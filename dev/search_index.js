var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = PositionalEmbeddings","category":"page"},{"location":"#PositionalEmbeddings","page":"Home","title":"PositionalEmbeddings","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for PositionalEmbeddings.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The PositionalEmbeddings package provides implementations of positional embeddings for encoding sequential position information into feature vectors. This encoding is essential for models where the order of sequence elements must be preserved during processing.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package implements two foundational approaches to positional encoding:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Rotary Position Embeddings (RoPE) encode positions by rotating vectors in 2D subspaces, enabling explicit relative position modeling through geometric transformations.\nAbsolute Positional Embeddings (AbsolutePE) create unique position markers using sinusoidal functions, following the original approach from \"Attention Is All You Need.\"","category":"page"},{"location":"#API-Reference","page":"Home","title":"API Reference","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Modules = [PositionalEmbeddings]","category":"page"},{"location":"#PositionalEmbeddings.AbsolutePE","page":"Home","title":"PositionalEmbeddings.AbsolutePE","text":"AbsolutePE{T<:AbstractArray}\nAbsolutePE(embedding_size::Int, max_length::Int; base::Number=10_000)\n\nAbsolute Position Embeddings using sinusoidal frequencies from \"Attention Is All You Need\" paper. Formula: PE(pos,2i) = sin(pos/10000^(2i/dmodel))         PE(pos,2i+1) = cos(pos/10000^(2i/dmodel))\n\nFields\n\nembedding_size::Int: Size of the embedding dimension (d_model)\nmax_length::Int: Maximum sequence length supported\nembeddings::T: Positional embeddings\n\n\n\n\n\n","category":"type"},{"location":"#PositionalEmbeddings.RoPE","page":"Home","title":"PositionalEmbeddings.RoPE","text":"RoPE(head_size::Int, seq_len::Int;\n    base::Number=10_000,\n    scale::Number=1.0)\n\nRotary Position Embeddings (RoPE) implementation as described in the paper \"RoFormer: Enhanced Transformer with Rotary Position Embedding\".\n\nConstruct a RoPE object with the following arguments:\n\nhead_size::Int: Head size to apply rotation to (must be multiple of 2)\nseq_len::Int: Maximum sequence length to support\nbase::Number=10_000: Base for geometric progression of frequencies\nscale::Number=1.0: Scaling factor for the frequencies\n\nExamples\n\n# Create RoPE for a model with 512 head size and max sequence length of 1024\nrope = RoPE(512, 1024)\n\n# Apply RoPE to input tensor of shape (head_size, seq_len, nheads*batch_size)\nQ = randn(Float32, 512, 100, 32)\nQ_positioned = rope(x)\n\n\n\n\n\n","category":"type"},{"location":"#PositionalEmbeddings.RoPE-Tuple{AbstractArray}","page":"Home","title":"PositionalEmbeddings.RoPE","text":"(rope::RoPE)(x) -> AbstractArray\n\nApply Rotary Position Embeddings to the input array x of shape (head_size, seq_len, batch * num_heads).\n\nArguments\n\nx: Input array where first dimension must match rope.head_size and second dimension must not exceed      the maximum cached sequence length.\n\nSee also: RoPE\n\n\n\n\n\n","category":"method"},{"location":"#PositionalEmbeddings.compute_frequencies","page":"Home","title":"PositionalEmbeddings.compute_frequencies","text":"compute_frequencies(dim::Int, seq_len::Int, base::Number=10_000)\n\nCompute frequency bands for rotary position embeddings.\n\nArguments\n\ndim::Int: Number of dimensions for the frequency bands\nseq_len::Int: Maximum sequence length to compute frequencies for\nbase::Number=10_000: Base for geometric progression of frequencies\n\nReturns\n\nMatrix of shape (dim, seq_len) containing frequency values\n\n\n\n\n\n","category":"function"},{"location":"#PositionalEmbeddings.create_causal_mask-Tuple{Int64}","page":"Home","title":"PositionalEmbeddings.create_causal_mask","text":"create_causal_mask(seq_len::Int)\n\nCreate a causal (autoregressive) attention mask that prevents positions from attending to future positions. This is commonly used in language models to ensure predictions only depend on previous tokens.\n\nThe mask ensures that position i can only attend to positions j ≤ i, creating a triangular pattern where the upper triangle including diagonal is masked (True) and the lower triangle is unmasked (False).\n\nArguments\n\nseq_len::Int: Length of the sequence to create mask for\n\nReturns\n\n3D boolean array of shape (seqlen, seqlen, 1) where True indicates positions to mask\n\nExamples\n\njulia> mask = create_causal_mask(3)[:,:,1]\n3×3 Matrix{Bool}:\n 1  1  1  # First position can't attend anything\n 0  1  1  # Second position can attend to first only\n 0  0  1  # Third position can attend to first and second\n\n\n\n\n\n","category":"method"},{"location":"#PositionalEmbeddings.create_padding_mask-Tuple{Vector{Int64}, Int64}","page":"Home","title":"PositionalEmbeddings.create_padding_mask","text":"create_padding_mask(lengths::Vector{Int}, max_len::Int)\n\nCreate padding masks for batched sequences of varying lengths. This ensures that padded positions (positions beyond each sequence's actual length) are masked out and don't participate in attention.\n\nArguments\n\nlengths::Vector{Int}: Actual length of each sequence in the batch\nmax_len::Int: Maximum sequence length (padded length)\n\nReturns\n\n3D boolean array of shape (batchsize, maxlen, 1) where True indicates padded positions\n\nExamples\n\n# For 2 sequences of lengths 2 and 3, padded to length 4:\njulia> mask = create_padding_mask([2, 3], 4)[:,:,1]\n2×4 Matrix{Bool}:\n 0  0  1  1  # First sequence: length 2, positions 3-4 are padding\n 0  0  0  1  # Second sequence: length 3, position 4 is padding\n\nUsage with Causal Mask\n\nPadding and causal masks are often combined for batched autoregressive tasks:\n\nseq_len = 5\nbatch_lengths = [3, 4]\n\n# Create both masks\ncausal = create_causal_mask(seq_len)                # Shape: (5, 5, 1)\npadding = create_padding_mask(batch_lengths, seq_len) # Shape: (2, 5, 1)\n\n# Combine masks which will either prevent attending to future tokens or padding tokens\ncombined = causal .| padding\n\n# final_mask will prevent:\n# 1. Attending to future tokens (from causal mask)\n# 2. Attending to padding tokens (from padding mask)\n\n\n\n\n\n","category":"method"},{"location":"#PositionalEmbeddings.neg_half-Tuple{AbstractArray}","page":"Home","title":"PositionalEmbeddings.neg_half","text":"neg_half(x::AbstractArray, dim::Int=1)\n\nHelper function that negates the second half of the array along dimension dim. This implementatio uses half negative array instead of interleaving pairs, as in LlaMA https://github.com/huggingface/transformers/issues/25199\n\nArguments\n\nx::AbstractArray: Input array\ndim::Int=1: Dimension along which to perform the operati    on\n\nReturns\n\nArray with second half negated along specified dimension\n\n\n\n\n\n","category":"method"},{"location":"#Usage-Examples","page":"Home","title":"Usage Examples","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"# Create RoPE for head dimension of 64 and maximum sequence length of 1024\nrope = RoPE(64, 1024)\n\n# Apply to input tensor of shape (head_size, seq_len, nheads*batch)\n# For example, with 64-dim heads, sequence length 100, 8 heads × 32 batch size:\nx = randn(Float32, 64, 100, 256)  # 256 = 8 heads × 32 batch\nx_with_positions = rope(x)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Input tensors for RoPE must follow the shape (headsize, seqlen, nheads*batch). The headsize parameter must be even, seqlen represents your sequence length, and the final dimension combines the number of attention heads and batch size.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The RoPE constructor accepts several parameters:","category":"page"},{"location":"","page":"Home","title":"Home","text":"function RoPE(head_size::Int, seq_len::Int;\n    base::Number=10_000,\n    scale::Number=1.0,\n    T::Type=Float32)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The base parameter controls frequency bands for position encoding, with higher values creating slower-changing position representations. The scale parameter allows adjusting the positional encoding's influence.","category":"page"},{"location":"#Absolute-Positional-Embeddings","page":"Home","title":"Absolute Positional Embeddings","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"AbsolutePE implements fixed positional patterns through sinusoidal encoding:","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Create embeddings for 512-dimensional features up to length 1000\npe = AbsolutePE(512, 1000)\n\n# Apply to input tensor of shape (seq_len, features, batch)\nx = randn(Float32, 100, 512, 32)\nx_with_positions = pe(x)","category":"page"},{"location":"","page":"Home","title":"Home","text":"For AbsolutePE, tensors require the shape (seqlen, features, batch), where features matches your model's dimension and seqlen represents the sequence length.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The AbsolutePE constructor allows customization through:","category":"page"},{"location":"","page":"Home","title":"Home","text":"function AbsolutePE(embedding_size::Int, max_length::Int; base::Number=10_000)","category":"page"},{"location":"","page":"Home","title":"Home","text":"(Image: AbsolutePE)","category":"page"},{"location":"","page":"Home","title":"Home","text":"The base parameter influences the wavelength pattern of sinusoidal embeddings, with each dimension using a different frequency derived from this base value.","category":"page"},{"location":"#Flux-Integration-Example","page":"Home","title":"Flux Integration Example","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This example that adds RoPERoPEMultiHeadAttention that. Here's the complete implementation:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using PositionalEmbeddings\nusing LinearAlgebra\nusing WeightInitializers\nusing Functors\nusing NNlib\n\nstruct RoPEMultiHeadAttention{T<:AbstractFloat, A<:AbstractArray{T, 2}}\n    Wq::A\n    Wk::A\n    Wv::A\n    Wo::A\n    num_heads::Int\n    head_dim::Int\n    scale::T\n    rope::RoPE\nend\n\nfunction RoPEMultiHeadAttention(d_model::Int, num_heads::Int; maxlen=1000)\n    head_dim = d_model ÷ num_heads\n    @assert head_dim * num_heads == d_model \"d_model ($d_model) must be divisible by num_heads ($num_heads)\"\n\n    Wq = kaiming_normal(d_model, d_model)\n    Wk = kaiming_normal(d_model, d_model)\n    Wv = kaiming_normal(d_model, d_model)\n    Wo = kaiming_normal(d_model, d_model)\n\n    scale = Float32(sqrt(head_dim))\n    rope = RoPE(head_dim, maxlen)\n\n    RoPEMultiHeadAttention(Wq, Wk, Wv, Wo, num_heads, head_dim, scale, rope)\nend\n\n# Split: (d_model, seqlen, batch) -> (head_dim, seqlen, num_heads * batch)\nfunction split_heads(x::AbstractArray, head_dim::Int, num_heads::Int)\n    d_model, seqlen, batch = size(x)\n    return reshape(permutedims(reshape(x, head_dim, num_heads, seqlen, batch), (1, 3, 2, 4)),\n                head_dim, seqlen, num_heads * batch)\nend\n\n# Join: (head_dim, seqlen, num_heads * batch) -> (d_model, seqlen, batch)\nfunction join_heads(x::AbstractArray, head_dim::Int, num_heads::Int, batch_size::Int)\n    return reshape(permutedims(reshape(x, head_dim, size(x, 2), num_heads, batch_size), (1, 3, 2, 4)),\n                head_dim * num_heads, size(x, 2), batch_size)\nend\n\nfunction apply_mask(logits, mask)\n    neginf = typemin(eltype(logits))\n    ifelse.(mask, logits, neginf)\nend\n\nfunction (mha::RoPEMultiHeadAttention)(x::AbstractArray, mask=nothing)\n    d_model, seqlen, batch_size = size(x)\n\n    # Project and split heads in one go\n    q = split_heads(reshape(mha.Wq * reshape(x, d_model, :), d_model, seqlen, batch_size),\n                mha.head_dim, mha.num_heads)\n    k = split_heads(reshape(mha.Wk * reshape(x, d_model, :), d_model, seqlen, batch_size),\n                mha.head_dim, mha.num_heads)\n    v = split_heads(reshape(mha.Wv * reshape(x, d_model, :), d_model, seqlen, batch_size),\n                mha.head_dim, mha.num_heads)\n\n    # Apply RoPE\n    q = mha.rope(q)\n    k = mha.rope(k)\n\n    # All operations now work with (head_dim, seqlen, num_heads * batch)\n    attention_scores = NNlib.batched_mul(NNlib.batched_transpose(k), (q ./ mha.scale))\n\n    if !isnothing(mask)\n        neginf = typemin(eltype(attention_scores))\n        attention_scores = ifelse.(mask, attention_scores, neginf)\n    end\n\n    attention_probs = softmax(attention_scores; dims=1)\n    attention_output = NNlib.batched_mul(v, attention_probs)\n\n    # Join heads only at the very end\n    output = join_heads(attention_output, mha.head_dim, mha.num_heads, batch_size)\n    return reshape(mha.Wo * reshape(output, d_model, :), d_model, seqlen, batch_size)\nend\nFunctors.@functor RoPEMultiHeadAttention\nx = rand(512, 20, 32);\nmha = RoPEMultiHeadAttention(512, 8)\nmha(x)","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
