<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Home · PositionalEmbeddings.jl</title><meta name="title" content="Home · PositionalEmbeddings.jl"/><meta property="og:title" content="Home · PositionalEmbeddings.jl"/><meta property="twitter:title" content="Home · PositionalEmbeddings.jl"/><meta name="description" content="Documentation for PositionalEmbeddings.jl."/><meta property="og:description" content="Documentation for PositionalEmbeddings.jl."/><meta property="twitter:description" content="Documentation for PositionalEmbeddings.jl."/><meta property="og:url" content="https://mashu.github.io/PositionalEmbeddings.jl/"/><meta property="twitter:url" content="https://mashu.github.io/PositionalEmbeddings.jl/"/><link rel="canonical" href="https://mashu.github.io/PositionalEmbeddings.jl/"/><script data-outdated-warner src="assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="assets/documenter.js"></script><script src="search_index.js"></script><script src="siteinfo.js"></script><script src="../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href><img src="assets/logo.svg" alt="PositionalEmbeddings.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href>PositionalEmbeddings.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li class="is-active"><a class="tocitem" href>Home</a><ul class="internal"><li><a class="tocitem" href="#API-Reference"><span>API Reference</span></a></li><li><a class="tocitem" href="#Usage-Examples"><span>Usage Examples</span></a></li><li><a class="tocitem" href="#Flux-Integration-Example"><span>Flux Integration Example</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Home</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Home</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/mashu/PositionalEmbeddings.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/mashu/PositionalEmbeddings.jl/blob/main/docs/src/index.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="PositionalEmbeddings"><a class="docs-heading-anchor" href="#PositionalEmbeddings">PositionalEmbeddings</a><a id="PositionalEmbeddings-1"></a><a class="docs-heading-anchor-permalink" href="#PositionalEmbeddings" title="Permalink"></a></h1><p>Documentation for <a href="https://github.com/mashu/PositionalEmbeddings.jl">PositionalEmbeddings</a>.</p><p>The PositionalEmbeddings package provides implementations of positional embeddings for encoding sequential position information into feature vectors. This encoding is essential for models where the order of sequence elements must be preserved during processing.</p><p>The package implements two foundational approaches to positional encoding:</p><ul><li><p>Rotary Position Embeddings (RoPE) encode positions by rotating vectors in 2D subspaces, enabling explicit relative position modeling through geometric transformations.</p></li><li><p>Absolute Positional Embeddings (AbsolutePE) create unique position markers using sinusoidal functions, following the original approach from &quot;Attention Is All You Need.&quot;</p></li></ul><h2 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PositionalEmbeddings.AbsolutePE" href="#PositionalEmbeddings.AbsolutePE"><code>PositionalEmbeddings.AbsolutePE</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">AbsolutePE{T&lt;:AbstractArray}
AbsolutePE(embedding_size::Int, max_length::Int; base::Number=10_000)</code></pre><p>Absolute Position Embeddings using sinusoidal frequencies from &quot;Attention Is All You Need&quot; paper. Formula: PE(pos,2i) = sin(pos/10000^(2i/d<em>model))         PE(pos,2i+1) = cos(pos/10000^(2i/d</em>model))</p><p><strong>Fields</strong></p><ul><li><code>embedding_size::Int</code>: Size of the embedding dimension (d_model)</li><li><code>max_length::Int</code>: Maximum sequence length supported</li><li><code>embeddings::T</code>: Positional embeddings</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mashu/PositionalEmbeddings.jl/blob/4504eea91be1b259f23315ed98dfc794ff0546c8/src/PositionalEmbeddings.jl#L26-L38">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PositionalEmbeddings.RoPE" href="#PositionalEmbeddings.RoPE"><code>PositionalEmbeddings.RoPE</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">RoPE(head_size::Int, seq_len::Int;
    base::Number=10_000,
    scale::Number=1.0)</code></pre><p>Rotary Position Embeddings (RoPE) implementation as described in the paper &quot;RoFormer: Enhanced Transformer with Rotary Position Embedding&quot;.</p><p>Construct a RoPE object with the following arguments:</p><ul><li><code>head_size::Int</code>: Head size to apply rotation to (must be multiple of 2)</li><li><code>seq_len::Int</code>: Maximum sequence length to support</li><li><code>base::Number=10_000</code>: Base for geometric progression of frequencies</li><li><code>scale::Number=1.0</code>: Scaling factor for the frequencies</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Create RoPE for a model with 512 head size and max sequence length of 1024
rope = RoPE(512, 1024)

# Apply RoPE to input tensor of shape (head_size, seq_len, nheads*batch_size)
Q = randn(Float32, 512, 100, 32)
Q_positioned = rope(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mashu/PositionalEmbeddings.jl/blob/4504eea91be1b259f23315ed98dfc794ff0546c8/src/PositionalEmbeddings.jl#L63-L86">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PositionalEmbeddings.RoPE-Tuple{AbstractArray}" href="#PositionalEmbeddings.RoPE-Tuple{AbstractArray}"><code>PositionalEmbeddings.RoPE</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">(rope::RoPE)(x) -&gt; AbstractArray</code></pre><p>Apply Rotary Position Embeddings to the input array <code>x</code> of shape <code>(head_size, seq_len, batch * num_heads)</code>.</p><p><strong>Arguments</strong></p><ul><li><code>x</code>: Input array where first dimension must match <code>rope.head_size</code> and second dimension must not exceed      the maximum cached sequence length.</li></ul><p>See also: <a href="#PositionalEmbeddings.RoPE"><code>RoPE</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mashu/PositionalEmbeddings.jl/blob/4504eea91be1b259f23315ed98dfc794ff0546c8/src/PositionalEmbeddings.jl#L129-L139">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PositionalEmbeddings.compute_frequencies" href="#PositionalEmbeddings.compute_frequencies"><code>PositionalEmbeddings.compute_frequencies</code></a> — <span class="docstring-category">Function</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">compute_frequencies(dim::Int, seq_len::Int, base::Number=10_000)</code></pre><p>Compute frequency bands for rotary position embeddings.</p><p><strong>Arguments</strong></p><ul><li><code>dim::Int</code>: Number of dimensions for the frequency bands</li><li><code>seq_len::Int</code>: Maximum sequence length to compute frequencies for</li><li><code>base::Number=10_000</code>: Base for geometric progression of frequencies</li></ul><p><strong>Returns</strong></p><ul><li>Matrix of shape (dim, seq_len) containing frequency values</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mashu/PositionalEmbeddings.jl/blob/4504eea91be1b259f23315ed98dfc794ff0546c8/src/PositionalEmbeddings.jl#L7-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PositionalEmbeddings.create_causal_mask-Tuple{Int64}" href="#PositionalEmbeddings.create_causal_mask-Tuple{Int64}"><code>PositionalEmbeddings.create_causal_mask</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">create_causal_mask(seq_len::Int)</code></pre><p>Create a causal (autoregressive) attention mask that prevents positions from attending to future positions. This is commonly used in language models to ensure predictions only depend on previous tokens.</p><p>The mask ensures that position i can only attend to positions j ≤ i, creating a triangular pattern where the upper triangle including diagonal is masked (True) and the lower triangle is unmasked (False).</p><p><strong>Arguments</strong></p><ul><li><code>seq_len::Int</code>: Length of the sequence to create mask for</li></ul><p><strong>Returns</strong></p><ul><li>3D boolean array of shape (seq<em>len, seq</em>len, 1) where True indicates positions to mask</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">julia&gt; mask = create_causal_mask(3)[:,:,1]
3×3 Matrix{Bool}:
 1  1  1  # First position can&#39;t attend anything
 0  1  1  # Second position can attend to first only
 0  0  1  # Third position can attend to first and second</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mashu/PositionalEmbeddings.jl/blob/4504eea91be1b259f23315ed98dfc794ff0546c8/src/PositionalEmbeddings.jl#L158-L181">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PositionalEmbeddings.create_padding_mask-Tuple{Vector{Int64}, Int64}" href="#PositionalEmbeddings.create_padding_mask-Tuple{Vector{Int64}, Int64}"><code>PositionalEmbeddings.create_padding_mask</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">create_padding_mask(lengths::Vector{Int}, max_len::Int)</code></pre><p>Create padding masks for batched sequences of varying lengths. This ensures that padded positions (positions beyond each sequence&#39;s actual length) are masked out and don&#39;t participate in attention.</p><p><strong>Arguments</strong></p><ul><li><code>lengths::Vector{Int}</code>: Actual length of each sequence in the batch</li><li><code>max_len::Int</code>: Maximum sequence length (padded length)</li></ul><p><strong>Returns</strong></p><ul><li>3D boolean array of shape (batch<em>size, max</em>len, 1) where True indicates padded positions</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs"># For 2 sequences of lengths 2 and 3, padded to length 4:
julia&gt; mask = create_padding_mask([2, 3], 4)[:,:,1]
2×4 Matrix{Bool}:
 0  0  1  1  # First sequence: length 2, positions 3-4 are padding
 0  0  0  1  # Second sequence: length 3, position 4 is padding</code></pre><p><strong>Usage with Causal Mask</strong></p><p>Padding and causal masks are often combined for batched autoregressive tasks:</p><pre><code class="nohighlight hljs">seq_len = 5
batch_lengths = [3, 4]

# Create both masks
causal = create_causal_mask(seq_len)                # Shape: (5, 5, 1)
padding = create_padding_mask(batch_lengths, seq_len) # Shape: (2, 5, 1)

# Combine masks which will either prevent attending to future tokens or padding tokens
combined = causal .| padding

# final_mask will prevent:
# 1. Attending to future tokens (from causal mask)
# 2. Attending to padding tokens (from padding mask)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mashu/PositionalEmbeddings.jl/blob/4504eea91be1b259f23315ed98dfc794ff0546c8/src/PositionalEmbeddings.jl#L186-L226">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PositionalEmbeddings.neg_half-Tuple{AbstractArray}" href="#PositionalEmbeddings.neg_half-Tuple{AbstractArray}"><code>PositionalEmbeddings.neg_half</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">neg_half(x::AbstractArray, dim::Int=1)</code></pre><p>Helper function that negates the second half of the array along dimension <code>dim</code>. This implementatio uses half negative array instead of interleaving pairs, as in LlaMA https://github.com/huggingface/transformers/issues/25199</p><p><strong>Arguments</strong></p><ul><li><code>x::AbstractArray</code>: Input array</li><li><code>dim::Int=1</code>: Dimension along which to perform the operati    on</li></ul><p><strong>Returns</strong></p><ul><li>Array with second half negated along specified dimension</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/mashu/PositionalEmbeddings.jl/blob/4504eea91be1b259f23315ed98dfc794ff0546c8/src/PositionalEmbeddings.jl#L109-L122">source</a></section></article><h2 id="Usage-Examples"><a class="docs-heading-anchor" href="#Usage-Examples">Usage Examples</a><a id="Usage-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Usage-Examples" title="Permalink"></a></h2><pre><code class="language-julia hljs"># Create RoPE for head dimension of 64 and maximum sequence length of 1024
rope = RoPE(64, 1024)

# Apply to input tensor of shape (head_size, seq_len, nheads*batch)
# For example, with 64-dim heads, sequence length 100, 8 heads × 32 batch size:
x = randn(Float32, 64, 100, 256)  # 256 = 8 heads × 32 batch
x_with_positions = rope(x)</code></pre><p>Input tensors for RoPE must follow the shape (head<em>size, seq</em>len, nheads*batch). The head<em>size parameter must be even, seq</em>len represents your sequence length, and the final dimension combines the number of attention heads and batch size.</p><p>The RoPE constructor accepts several parameters:</p><pre><code class="language-julia hljs">function RoPE(head_size::Int, seq_len::Int;
    base::Number=10_000,
    scale::Number=1.0,
    T::Type=Float32)</code></pre><p>The base parameter controls frequency bands for position encoding, with higher values creating slower-changing position representations. The scale parameter allows adjusting the positional encoding&#39;s influence.</p><h3 id="Absolute-Positional-Embeddings"><a class="docs-heading-anchor" href="#Absolute-Positional-Embeddings">Absolute Positional Embeddings</a><a id="Absolute-Positional-Embeddings-1"></a><a class="docs-heading-anchor-permalink" href="#Absolute-Positional-Embeddings" title="Permalink"></a></h3><p>AbsolutePE implements fixed positional patterns through sinusoidal encoding:</p><pre><code class="language-julia hljs"># Create embeddings for 512-dimensional features up to length 1000
pe = AbsolutePE(512, 1000)

# Apply to input tensor of shape (seq_len, features, batch)
x = randn(Float32, 100, 512, 32)
x_with_positions = pe(x)</code></pre><p>For AbsolutePE, tensors require the shape (seq<em>len, features, batch), where features matches your model&#39;s dimension and seq</em>len represents the sequence length.</p><p>The AbsolutePE constructor allows customization through:</p><pre><code class="language-julia hljs">function AbsolutePE(embedding_size::Int, max_length::Int; base::Number=10_000)</code></pre><p><img src="assets/AbsolutePE-128-100.svg" alt="AbsolutePE"/></p><p>The base parameter influences the wavelength pattern of sinusoidal embeddings, with each dimension using a different frequency derived from this base value.</p><h2 id="Flux-Integration-Example"><a class="docs-heading-anchor" href="#Flux-Integration-Example">Flux Integration Example</a><a id="Flux-Integration-Example-1"></a><a class="docs-heading-anchor-permalink" href="#Flux-Integration-Example" title="Permalink"></a></h2><p>This example that adds <code>RoPERoPEMultiHeadAttention</code> that. Here&#39;s the complete implementation:</p><pre><code class="language-julia hljs">using PositionalEmbeddings
using LinearAlgebra
using WeightInitializers
using Functors
using NNlib

struct RoPEMultiHeadAttention{T&lt;:AbstractFloat, A&lt;:AbstractArray{T, 2}}
    Wq::A
    Wk::A
    Wv::A
    Wo::A
    num_heads::Int
    head_dim::Int
    scale::T
    rope::RoPE
end

function RoPEMultiHeadAttention(d_model::Int, num_heads::Int; maxlen=1000)
    head_dim = d_model ÷ num_heads
    @assert head_dim * num_heads == d_model &quot;d_model ($d_model) must be divisible by num_heads ($num_heads)&quot;

    Wq = kaiming_normal(d_model, d_model)
    Wk = kaiming_normal(d_model, d_model)
    Wv = kaiming_normal(d_model, d_model)
    Wo = kaiming_normal(d_model, d_model)

    scale = Float32(sqrt(head_dim))
    rope = RoPE(head_dim, maxlen)

    RoPEMultiHeadAttention(Wq, Wk, Wv, Wo, num_heads, head_dim, scale, rope)
end

# Split: (d_model, seqlen, batch) -&gt; (head_dim, seqlen, num_heads * batch)
function split_heads(x::AbstractArray, head_dim::Int, num_heads::Int)
    d_model, seqlen, batch = size(x)
    return reshape(permutedims(reshape(x, head_dim, num_heads, seqlen, batch), (1, 3, 2, 4)),
                head_dim, seqlen, num_heads * batch)
end

# Join: (head_dim, seqlen, num_heads * batch) -&gt; (d_model, seqlen, batch)
function join_heads(x::AbstractArray, head_dim::Int, num_heads::Int, batch_size::Int)
    return reshape(permutedims(reshape(x, head_dim, size(x, 2), num_heads, batch_size), (1, 3, 2, 4)),
                head_dim * num_heads, size(x, 2), batch_size)
end

function apply_mask(logits, mask)
    neginf = typemin(eltype(logits))
    ifelse.(mask, logits, neginf)
end

function (mha::RoPEMultiHeadAttention)(x::AbstractArray, mask=nothing)
    d_model, seqlen, batch_size = size(x)

    # Project and split heads in one go
    q = split_heads(reshape(mha.Wq * reshape(x, d_model, :), d_model, seqlen, batch_size),
                mha.head_dim, mha.num_heads)
    k = split_heads(reshape(mha.Wk * reshape(x, d_model, :), d_model, seqlen, batch_size),
                mha.head_dim, mha.num_heads)
    v = split_heads(reshape(mha.Wv * reshape(x, d_model, :), d_model, seqlen, batch_size),
                mha.head_dim, mha.num_heads)

    # Apply RoPE
    q = mha.rope(q)
    k = mha.rope(k)

    # All operations now work with (head_dim, seqlen, num_heads * batch)
    attention_scores = NNlib.batched_mul(NNlib.batched_transpose(k), (q ./ mha.scale))

    if !isnothing(mask)
        neginf = typemin(eltype(attention_scores))
        attention_scores = ifelse.(mask, attention_scores, neginf)
    end

    attention_probs = softmax(attention_scores; dims=1)
    attention_output = NNlib.batched_mul(v, attention_probs)

    # Join heads only at the very end
    output = join_heads(attention_output, mha.head_dim, mha.num_heads, batch_size)
    return reshape(mha.Wo * reshape(output, d_model, :), d_model, seqlen, batch_size)
end
Functors.@functor RoPEMultiHeadAttention
x = rand(512, 20, 32);
mha = RoPEMultiHeadAttention(512, 8)
mha(x)</code></pre><ul><li><a href="#PositionalEmbeddings.AbsolutePE"><code>PositionalEmbeddings.AbsolutePE</code></a></li><li><a href="#PositionalEmbeddings.RoPE-Tuple{AbstractArray}"><code>PositionalEmbeddings.RoPE</code></a></li><li><a href="#PositionalEmbeddings.RoPE"><code>PositionalEmbeddings.RoPE</code></a></li><li><a href="#PositionalEmbeddings.compute_frequencies"><code>PositionalEmbeddings.compute_frequencies</code></a></li><li><a href="#PositionalEmbeddings.create_causal_mask-Tuple{Int64}"><code>PositionalEmbeddings.create_causal_mask</code></a></li><li><a href="#PositionalEmbeddings.create_padding_mask-Tuple{Vector{Int64}, Int64}"><code>PositionalEmbeddings.create_padding_mask</code></a></li><li><a href="#PositionalEmbeddings.neg_half-Tuple{AbstractArray}"><code>PositionalEmbeddings.neg_half</code></a></li></ul></article><nav class="docs-footer"><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Wednesday 4 December 2024 20:20">Wednesday 4 December 2024</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
